fractal20220817_data
FeaturesDict({
    'aspects': FeaturesDict({
        'already_success': bool,
        'feasible': bool,
        'has_aspects': bool,
        'success': bool,
        'undesirable': bool,
    }),
    'attributes': FeaturesDict({
        'collection_mode': int64,
        'collection_mode_name': string,
        'data_type': int64,
        'data_type_name': string,
        'env': int64,
        'env_name': string,
        'location': int64,
        'location_name': string,
        'objects_family': int64,
        'objects_family_name': string,
        'task_family': int64,
        'task_family_name': string,
    }),
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=rpy commanded orientation displacement, in base-relative frame),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32, description=commanded end-effector displacement, in base-relative frame),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'base_pose_tool_reached': Tensor(shape=(7,), dtype=float32, description=end-effector base-relative position+quaternion pose),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_commanded': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32, description=height of end-effector from ground),
            'image': Image(shape=(256, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'orientation_box': Tensor(shape=(2, 3), dtype=float32),
            'orientation_start': Tensor(shape=(4,), dtype=float32),
            'robot_orientation_positions_box': Tensor(shape=(3, 3), dtype=float32),
            'rotation_delta_to_go': Tensor(shape=(3,), dtype=float32, description=rotational displacement from current orientation to target),
            'src_rotation': Tensor(shape=(4,), dtype=float32),
            'vector_to_go': Tensor(shape=(3,), dtype=float32, description=displacement from current end-effector position to target),
            'workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
kuka
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'clip_function_input/base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
            'clip_function_input/workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32),
            'image': Image(shape=(512, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'task_id': Tensor(shape=(1,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
    'success': bool,
})
bridge
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'open_gripper': bool,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'state': Tensor(shape=(7,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
taco_play
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'actions': Tensor(shape=(7,), dtype=float32, description=absolute desired values for gripper pose (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_gripper': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the gripper camera frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_world': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the robot base frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'terminate_episode': float32,
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'depth_gripper': Tensor(shape=(84, 84), dtype=float32),
            'depth_static': Tensor(shape=(150, 200), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'rgb_gripper': Image(shape=(84, 84, 3), dtype=uint8),
            'rgb_static': Image(shape=(150, 200, 3), dtype=uint8, description=RGB static image of shape. (150, 200, 3). Subsampled from (200,200, 3) image.),
            'robot_obs': Tensor(shape=(15,), dtype=float32, description=EE position (3), EE orientation in euler angles (3), gripper width (1), joint positions (7), gripper action (1)),
            'structured_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
jaco_play
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'end_effector_cartesian_pos': Tensor(shape=(7,), dtype=float32),
            'end_effector_cartesian_velocity': Tensor(shape=(6,), dtype=float32),
            'image': Image(shape=(224, 224, 3), dtype=uint8),
            'image_wrist': Image(shape=(224, 224, 3), dtype=uint8),
            'joint_pos': Tensor(shape=(8,), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
berkeley_cable_routing
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity about the z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(7,), dtype=float32),
            'top_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist225_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist45_image': Image(shape=(128, 128, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
roboturk
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'front_rgb': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
nyu_door_opening_surprising_effectiveness
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity around x, y and z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(720, 960, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
viola
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'agentview_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by workspace camera),
            'ee_states': Tensor(shape=(16,), dtype=float32, description=Pose of the end effector specified as a homogenous matrix.),
            'eye_in_hand_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by in hand camera),
            'gripper_states': Tensor(shape=(1,), dtype=float32, description=gripper_states = 0 means the gripper is fully closed. The value represents the gripper width of Franka Panda Gripper.),
            'joint_states': Tensor(shape=(7,), dtype=float32, description=joint values),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
berkeley_autolab_ur5
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'image_with_depth': Image(shape=(480, 640, 1), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(15,), dtype=float32, description=Explanation of the robot state can be found at https://sites.google.com/corp/view/berkeley-ur5),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
toto
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'open_gripper': bool,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'state': Tensor(shape=(7,), dtype=float32, description=numpy array of shape (7,). Contains the robot joint states (as absolute joint angles) at each timestep),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
language_table
FeaturesDict({
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float32, description=An action representing a 2D delta cartesian setpoint for the robot.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'effector_target_translation': Tensor(shape=(2,), dtype=float32, description=The target location for the end effector as a 2D cartesian setpoint.),
            'effector_translation': Tensor(shape=(2,), dtype=float32, description=The current end effector translation as a 2D cartesian coordinate.),
            'instruction': Tensor(shape=(512,), dtype=int32, description=A UTF-8 encoded string as bytes.),
            'rgb': Image(shape=(360, 640, 3), dtype=uint8, description=An RGB image of the scene.),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
columbia_cairlab_pusht_real
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(2,), dtype=float32, description=Robot end effector XY state),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
stanford_kuka_multimodal_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x EEF position, 1x gripper open/close].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'contact': Tensor(shape=(50,), dtype=float32, description=Robot contact information.),
            'depth_image': Tensor(shape=(128, 128, 1), dtype=float32, description=Main depth camera observation.),
            'ee_forces_continuous': Tensor(shape=(50, 6), dtype=float32, description=Robot end-effector forces.),
            'ee_orientation': Tensor(shape=(4,), dtype=float32, description=Robot end-effector orientation quaternion.),
            'ee_orientation_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector orientation velocity.),
            'ee_position': Tensor(shape=(3,), dtype=float32, description=Robot end-effector position.),
            'ee_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector velocity.),
            'ee_yaw': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw.),
            'ee_yaw_delta': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw delta.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=Robot joint positions.),
            'joint_vel': Tensor(shape=(7,), dtype=float32, description=Robot joint velocities.),
            'optical_flow': Tensor(shape=(128, 128, 2), dtype=float32, description=Optical flow.),
            'state': Tensor(shape=(8,), dtype=float32, description=Robot proprioceptive information, [7x joint pos, 1x gripper open/close].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
nyu_rot_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot end effector delta positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot end effector positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
stanford_hydra_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x EEF positional delta, 
                            3x EEF orientation delta in euler angle, 1x close gripper].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_dense': Scalar(shape=(), dtype=bool, description=True if state is a waypoint(010) or in dense mode(x111).),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(27,), dtype=float32, description=Robot state, consists of [3x EEF position,4x EEF orientation in quaternion,
                            3x EEF orientation in euler angle,7x robot joint angles, 
                            7x robot joint velocities,3x gripper state.),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
austin_buds_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [6x end effector delta pose, 1x gripper position].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(24,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position, 16x robot end-effector homogeneous matrix].),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
nyu_franka_play_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(15,), dtype=float32, description=Robot action, consists of [7x joint velocities, 3x EE delta xyz, 3x EE delta rpy, 1x gripper position, 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth': Tensor(shape=(128, 128, 1), dtype=int32, description=Right camera depth observation.),
            'depth_additional_view': Tensor(shape=(128, 128, 1), dtype=int32, description=Left camera depth observation.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Right camera RGB observation.),
            'image_additional_view': Image(shape=(128, 128, 3), dtype=uint8, description=Left camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 3x EE xyz, 3x EE rpy.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
maniskill_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x end effector delta target position, 3x end effector delta target orientation in axis-angle format, 1x gripper target position (mimic for two fingers)]. For delta target position, an action of -1 maps to a robot movement of -0.1m, and action of 1 maps to a movement of 0.1m. For delta target orientation, its encoded angle is mapped to a range of [-0.1rad, 0.1rad] for robot execution. For example, an action of [1, 0, 0] means rotating along the x-axis by 0.1 rad. For gripper target position, an action of -1 means close, and an action of 1 means open.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'base_pose': Tensor(shape=(7,), dtype=float32, description=Robot base pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. The first three dimensions represent xyz positions in meters. The last four dimensions are the quaternion representation of rotation.),
            'depth': Image(shape=(256, 256, 1), dtype=uint16, description=Main camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'image': Image(shape=(256, 256, 3), dtype=uint8, description=Main camera RGB observation.),
            'main_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the main camera frame to the world frame in OpenGL/Blender convention.),
            'main_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Main camera extrinsic matrix in OpenCV convention.),
            'main_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Main camera intrinsic matrix in OpenCV convention.),
            'state': Tensor(shape=(18,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 2x gripper position, 7x robot joint angle velocity, 2x gripper velocity]. Angle in radians, position in meters.),
            'target_object_or_part_final_pose': Tensor(shape=(7,), dtype=float32, description=The final pose towards which the target object or object part needs be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. An episode is considered successful if the target object or object part is manipulated to this pose.),
            'target_object_or_part_final_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_final_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_final_pose). "Invalid" means that there is no success check on the final pose of target object or object part in the corresponding dimensions.),
            'target_object_or_part_initial_pose': Tensor(shape=(7,), dtype=float32, description=The initial pose of the target object or object part to be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. This variable is used to specify the target object or object part when multiple objects or object parts are present in an environment),
            'target_object_or_part_initial_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_initial_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_initial_pose).),
            'tcp_pose': Tensor(shape=(7,), dtype=float32, description=Robot tool-center-point pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. Tool-center-point is the center between the two gripper fingers.),
            'wrist_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the wrist camera frame to the world frame in OpenGL/Blender convention.),
            'wrist_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Wrist camera extrinsic matrix in OpenCV convention.),
            'wrist_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Wrist camera intrinsic matrix in OpenCV convention.),
            'wrist_depth': Image(shape=(256, 256, 1), dtype=uint16, description=Wrist camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'wrist_image': Image(shape=(256, 256, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
cmu_franka_exploration_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [end effector position3x, end effector orientation3x, gripper action1x, episode termination1x].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'highres_image': Image(shape=(480, 640, 3), dtype=uint8, description=High resolution main camera observation),
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
        'structured_action': Tensor(shape=(8,), dtype=float32, description=Structured action, consisting of hybrid affordance and end-effector control, described in Structured World Models from Human Videos.),
    }),
})
ucsd_kitchen_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=8-dimensional action, consisting of end-effector position and orientation, gripper open/close and a episode termination action.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(21,), dtype=float32, description=21-dimensional joint states, consists of robot joint angles, joint velocity and joint torque.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
ucsd_pick_and_place_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'disclaimer': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
        'n_transitions': Scalar(shape=(), dtype=int32, description=Number of transitions in the episode.),
        'success': Scalar(shape=(), dtype=bool, description=True if the last state of an episode is a success state, False otherwise.),
        'success_labeled_by': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x gripper velocities,1x gripper open/close torque].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x gripper position,3x gripper orientation, 1x finger distance].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
austin_sailor_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [3x robot ee pos, 3x ee quat, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information (not used in original SAILOR dataset).),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=True on last step of the episode.),
    }),
})
austin_sirius_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'action_mode': Tensor(shape=(1,), dtype=float32, description=Type of interaction. -1: initial human demonstration. 1: intervention. 0: autonomuos robot execution (includes pre-intervention class)),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'intv_label': Tensor(shape=(1,), dtype=float32, description=Same as action_modes, except 15 timesteps preceding intervention are labeled as -10.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [7x robot joint state, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information.),
            'wrist_image': Image(shape=(84, 84, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
bc_z
FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'future/axis_angle_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the rotation. Each action is a 3D delta to add to the current axis angle.),
            'future/target_close': Tensor(shape=(10,), dtype=int64, description=The next 10 actions for the gripper. Each action is the value the gripper closure should be changed to (notably it is *not* a delta.)),
            'future/xyz_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the positions. Each action is a 3D delta to add to current position.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'episode_success': float32,
            'image': Image(shape=(171, 213, 3), dtype=uint8, description=Camera image of the robot, downsampled 3x),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32, description=An embedding of the task via Universal Sentence Encoder (https://tfhub.dev/google/universal-sentence-encoder/4)),
            'natural_language_instruction': string,
            'present/autonomous': int64,
            'present/axis_angle': Tensor(shape=(3,), dtype=float32, description=The current rotation of the end effector in axis-angle representation.),
            'present/intervention': int64,
            'present/sensed_close': Tensor(shape=(1,), dtype=float32, description=How much the gripper is currently closed. Scaled from 0 to 1, but not all values from 0 to 1 are reachable. The range in the data is about 0.2 to 1),
            'present/xyz': Tensor(shape=(3,), dtype=float32, description=The current position of the end effector in axis-angle representation, in robot frame),
            'sequence_length': int64,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
usc_cloth_sim_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of x,y,z goal and picker commandpicker<0.5 = open, picker>0.5 = close.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(32, 32, 3), dtype=uint8, description=Image observation of cloth.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward as a normalized performance metric in [0, 1].0 = no change from initial state. 1 = perfect fold.-ve performance means the cloth is worse off than initial state.),
    }),
})
utokyo_pr2_opening_fridge_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})
furniture_bench_dataset_converted_externally_to_rlds
FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
        'furniture': Text(shape=(), dtype=string),
        'initial_randomness': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x eef pos velocities, 4x eef quat velocities, 1x gripper velocity].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(35,), dtype=float32, description=Robot state, consists of [3x eef position, 4x eef quaternion, 3x eef linear velocity, 3x eef angular velocity, 7x joint position, 7x joint velocity, 7x joint torque, 1x gripper width].),
            'wrist_image': Image(shape=(224, 224, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=+1 reward for each two-part assembly.),
        'skill_completion': Scalar(shape=(), dtype=float32, description=+1 skill completion reward; otherwise, 0.),
    }),
})


fmb
Error processing dataset fmb: Could not load dataset info from gs://gresearch/robotics/fmb/0.1.0/dataset_info.json
io_ai_tech
Error processing dataset io_ai_tech: Could not load dataset info from gs://gresearch/robotics/io_ai_tech/0.1.0/dataset_info.json
mimic_play
Error processing dataset mimic_play: Could not load dataset info from gs://gresearch/robotics/mimic_play/0.1.0/dataset_info.json
aloha_mobile
Error processing dataset aloha_mobile: Could not load dataset info from gs://gresearch/robotics/aloha_mobile/0.1.0/dataset_info.json
robo_set
Error processing dataset robo_set: Could not load dataset info from gs://gresearch/robotics/robo_set/0.1.0/dataset_info.json
tidybot
Error processing dataset tidybot: Could not load dataset info from gs://gresearch/robotics/tidybot/0.1.0/dataset_info.json
vima_converted_externally_to_rlds
Error processing dataset vima_converted_externally_to_rlds: Could not load dataset info from gs://gresearch/robotics/vima_converted_externally_to_rlds/0.1.0/dataset_info.json
spoc
Error processing dataset spoc: Could not load dataset info from gs://gresearch/robotics/spoc/0.1.0/dataset_info.json
plex_robosuite
Error processing dataset plex_robosuite: Could not load dataset info from gs://gresearch/robotics/plex_robosuite/0.1.0/dataset_info.json
